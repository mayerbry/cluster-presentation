<link rel="stylesheet" href="http://yandex.st/highlightjs/7.3/styles/default.min.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
<script src="http://yandex.st/highlightjs/7.3/highlight.min.js"></script>
<script>
$(document).ready(function() {
  $('pre code').each(function(i, e) {hljs.highlightBlock(e)});
});
</script>
<style>
pre code.bash {
  background: black;
  color: white;
  font-size: 1em;

}
</style>

Examples of parallel computing and using the cluster
========================================================
width: 1440
height: 900
transition: none
font-family: 'Helvetica'
css: my_style.css
title: "Examples of parallel computing and using the cluster"
author: Bryan Mayer
date: 2/4/2016



Topics
========================================================
1. Example background: a simulation study in CMV
2. Local machine parallel computing
3. Working on the remote machine
4. Sending jobs to the cluster


Simulating infection
========================================================
Need a figure here


Model code
========================================================
```{r, warning = F, message = F}
library(knitr)
library(ggplot2) 
theme_set(theme_classic())

source("Code/model_functions.R") #model functions in here
```

Example of a simulation:
========================================================
```{r, warning = F, message = F, fig.width = 12, fig.height = 4, fig.align='center'}
start_time = Sys.time()
example = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.5, parms)
print(Sys.time() - start_time)

ggplot(data = example, aes(x = time, y = viral_load)) + geom_line()

```

Multiple simulations
========================================================
```{r, warning = F, message = F, fig.width = 12, fig.height = 4, fig.align='center', cache = T}
total_simulations = 10

start_time = Sys.time()
example_simulations = NULL
for(run in 1:total_simulations){
  results = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.1, parms)
  results$run = run #keep track of the run
  example_simulations = rbind(example_simulations, results)
}
print(Sys.time() - start_time)

ggplot(data = example_simulations, 
       aes(x = time, y = viral_load, colour = factor(run))) + 
  geom_line()

```
Note: Looping and stacking is inefficient in R.    

Multiple simulations
========================================================
```{r, warning = F, message = F, echo = T, cache = T}
total_simulations = 100

start_time = Sys.time()
example_simulations = NULL
for(run in 1:total_simulations){
  results = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.1, parms)
  results$run = run #keep track of the run
  example_simulations = rbind(example_simulations, results)
}
print(Sys.time() - start_time)

rm(example_simulations)
```
If this scaled linearly, we would've expected about 5 seconds but this approach gets worse and worse as the data.frame increases in size, 20x longer per 10x increase - A run of 1000 took 5 minutes, 600x longer for a 100x increase.

Multiple simulations
========================================================
The actual simulations requirements:
```{r, warning = F, message = F, echo = T}
total_simulations = 1000
I0s = c(1:10, 15, 10 * 2:9, 1:4 * 10^2)
infectivity_set = c(1.01, 1.05, 1.1, seq(1.15, 1.3, 0.05), 
                    seq(1.35, 1.65, 0.05), seq(1.7, 2.15, 0.05))

est_time = 5 * length(I0s) * length(infectivity_set)/24
print(paste(est_time, "days"))

```

Local machine solutions
========================================================
1. The `doParallel` package
2. The `plyr` package (uses doParallel)

the doParallel package
========================================================

https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf
```{r, warning = F}
library(doParallel)
```
How many cores do I have?
```{r, warning = F}
detectCores()
```
How many is R using (1 is default)?
```{r, warning = F}
getDoParWorkers()
```
Register your cores and check:
```{r, warning = F}
registerDoParallel(2)
getDoParWorkers()
```

Using foreach
========================================================
id: foreach_sim

```{r, warning = F, message = F, echo = T, cache = T}
total_simulations = 1000

start_time = Sys.time()
example_simulations =  foreach(run = 1:total_simulations, .combine=rbind) %dopar% {
  results = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.1, parms,
                                    seed_set = 5)
  results$run = run #keep track of the run
  results
}
print(Sys.time() - start_time)

rm(example_simulations)
```
Versus 5 minutes, so pretty good improvement: partially from more efficient stacking using `.combine` in `foreach`


The plyr package
========================================================

The `plyr` package contains a set of functions that stacks R objects more efficiently. The functions follow the `apply` method of programming (which is just looping).
```{r}
?apply
```
- We will focus on `ldply`, a function that carries out a loop and lets us stack data frames
- In the following example, `ldply` takes two arguments: 1) a set of values to loop through (1:5) and a function that takes the values as a parameter (`i` here). The function just returns the input and `ldply` will stack those and return a data.frame .
```{r}
plyr::ldply(1:5, function(i) i)
```

Biostat example
========================================================

Fake data set has randomly assigned measurements of IgG and IgA. Treatment measurements are an average 2 units higher.
```{r, warning = F}
ptid = LETTERS[1:20]
immuno = c("IgG", "IgA")
trt = c("control", "treatment")
test_data = expand.grid(ptid = ptid, immuno = immuno, trt = trt)
test_data$outcome = with(test_data, ifelse(trt == "control", rnorm(40, 2), rnorm(40, 4)))
head(test_data)
```

Biostat example
========================================================

Run a t-test for both immunoglobulins and save all the results together.
```{r, warning = F, fig.align='center'}
ttest_results = plyr::ldply(immuno, function(imm){
  temp = with(subset(test_data, immuno == imm), t.test(outcome ~ trt))
  data.frame(
    immuno = imm,
    mean_diff = diff(temp$estimate),
    lower_ci = -temp$conf.int[2],
    upper_ci = -temp$conf.int[1]
  )
})
ttest_results
```

Biostat example
========================================================
```{r, warning = F, fig.align='center'}

ggplot(data = ttest_results, 
       aes(x = immuno, y = mean_diff, ymin = lower_ci, ymax = upper_ci)) +
  geom_point() +
  geom_errorbar(width = 0.2)

```


Back to simulation example - using plyr
========================================================
```{r, warning = F, message = F, echo = T, cache = T}
total_simulations = 1000

start_time = Sys.time()
example_simulations = plyr::ldply(1:total_simulations, function(run){ #ldply is a loop
  results = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.1, parms,
                                    seed_set = 5)
  results$run = run #keep track of the run
  results
})
print(Sys.time() - start_time)

rm(example_simulations)
```
This did even better than foreach without any parallel processing!

Adding parallel computing in plyr is simple.
========================================================
id: plyr_sim

```{r, warning = F, message = F, echo = T, cache = T}
total_simulations = 1000

start_time = Sys.time()
example_simulations = plyr::ldply(1:total_simulations, function(run){ #ldply is like a loop
  results = stochastic_model_latent(max_time = 100, initI = 10, infectivity = 1.1, parms,
                                    seed_set = 5)
  results$run = run #keep track of the run
  results
}, .parallel = T) #Just set this .parallel = T after you close the function

print(Sys.time() - start_time)

rm(example_simulations)
```
About 2x improvement.

Connecting to the server
========================================================
I use the rhino servers through scicomp. It requires an account and a "fred" drive.
https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/How%20to%20get%20or%20use%20an%20account.aspx

I will be doing tasks using Bash in the command line (Terminal app on a mac). For windows, you can use PuTTY (it is free):
http://sharedresources.fredhutch.org/libresources/putty

Any code with black background is command line code.

Connecting to rhino - through the command line
========================================================
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
ssh bmayer@rhino.fhcrc.org
``` 
and then enter your password when prompted

Then this might happen (enter yes)

![alt text](images/connect.png)

Connecting to rhino - using PuTTY (download)
========================================================

Download PuTTY:
http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html
The red box is what I downloaded.    
![alt text](images/putty_install.png)

Connecting to rhino - using PuTTY (connecting)
========================================================

Most of the settings are already set, just fill in the server and click Open.    
![alt text](images/putty_connect.png)

Connecting to rhino - using PuTTY (connecting)
========================================================
 Then click yes.    
![alt text](images/putty_connect2.png)    
And then login.    
![alt text](images/putty_login.png)

All of code in the black boxes is meant to be used in the command line and should work using PuTTY.


Create the remote directory
========================================================


```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
mkdir cluster-example
mkdir installed_packages
```

Make this directory to remotely install R packages (next slide)
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
mkdir cluster-example
cd cluster-example
mkdir installed_packages
```

Open R
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
R
```

To quit R:
```{r, eval = F} 
quit()
```


How to check if a package is available.
========================================================
Check its version: will throw an error if the package is not available.
```{r, eval = F} 
packageVersion("ggplot2")
```
ggplot2 is installed but it's an old version. So let's install and use a more updated version:
```{r, eval = F} 
install.packages("ggplot2", lib = "installed_packages/") 
library(ggplot2, lib.loc = "installed_packages/")
packageVersion("ggplot2")
```

Note: When you load R on the server, it will always set your working directory to the source file locations. So all paths are relative to that. Get the absolute path with `getwd()`.


Transferring files from local machine to the remote machine
========================================================
Four options:    
1) Use the command line (from the local computer transferring to the server in the correct directory). This can be a pain and will require a password if you don't set some advanced settings.
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
scp model_functions.R bmayer@rhino.fhcrc.org:/home/bmayer/cluster-example/
```

2) Use an ftp client (there are a few free ones for windows).
https://cyberduck.io/?l=en

3) Use NoMachine: remotely connects to a linux machine with graphical interface
https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/Connecting%20to%20Session%20Server%20from%20Mac.aspx

4) Do your programming from the shell using a text editor like vim or emacs (very high learning curve).

Using Cyberduck
========================================================
After Cyberduck is installed click on "Open Connection". Select "SFTP (SSH File Transfer Protocol)" from the dropdown. Enter the server name (rhino.fhcrc.org) , leave the port as 22, and then enter your hutchnet ID and password. Click connect.

![alt text](images/cyberduck_login.png)

Using Cyberduck
========================================================
This is what the interface looks like (on a Mac) after connected. You can drag and drop files and folders between your local machine and your server directory.

![alt text](images/cyberduck_interface.png)

Using R on the server
========================================================
When using R through the command line, a lot of convienent functionality is lost. For example, it is no longer easy to highlight and run chunks of code. However, using R on the server frees up your local machine for other tasks if you are conducting computational intensive processses (like using all of your cores for parallel processing).    

One option to continue using the interactive console, is to utilize the `source` function to read in scripts that you can edit outside of R then pass onto your remote drive using software like Cyberduck.

Grabbing nodes
========================================================

https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/Grab%20Commands.aspx

```{r, echo = F}
node_cmds = c("grabfullnode", "12 processors on one node",
"grabnode", "6 processors on one node",
"grabhalfnode", "6 processors on one node (same as grabnode)",
"grabquarternode", "3 processors one one node",
"grabcpu", '1 processor',
"grabR",' 1 processor (starts an R shell)',
"grablargenode", "32 processors on a large-memory node")
node_table = as.data.frame(matrix(node_cmds, nrow = 7, byrow = T))
kable(node_table, col.names = c("bash command", "total cores"))

```

Grabbing nodes (grabnode command)
========================================================
Just going to use grabnode for this example. These commands don't actually grant you access (you already have access to these cores) but they tell the server what your intentions are. The link contains information on server etiquette: do not request more cores in your R session than you grabbed.

```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
grabnode    
```
![alt text](images/grabnode.png)     

You may have to accept key changes like when you first log on to the server.

Running R scripts on the server
========================================================
The general code to run an R script on the server:
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
R CMD BATCH myR_script.R
```

This will create a myR.script.Rout (default naming) which contains an echo of all the code, results that were printed, and a process time statement at the end. You can check the file from the command line with:
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
less myR_script.Rout
```


To suppress the code echo and only output printed results add the `--slave` option. We also can add a second file name (results.txt) to name the output file.
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
R CMD BATCH --slave myR_script.R results.txt
```

Running the simulation script
========================================================
[foreach example](#/foreach_sim)    
[plyr example](#/plyr_sim)

Add the following to scripts to the server directory (for me "/users/bmayer/cluster-example/" from earlier):    
[model function code](https://github.fhcrc.org/bmayer/cluster-presentation/blob/master/Code/model_functions.R)    
[a script to run simulations](https://github.fhcrc.org/bmayer/cluster-presentation/blob/master/Code/test_server_code.R)

```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
R CMD BATCH --slave test_server_code.R results.txt
less results.txt
```

Performance of simulation script
========================================================

```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
less results.txt
```
![alt text](images/results.png)    
plyr performance is far superior and it scaled closely to the estimates from the earlier simulations. 

Note that the results were only performance checks and that nothing from the model simulations was saved to be used. Alternatively, this script could've been run within R with `source("test_server_code.R")` and then example_simulations would be available for analysis in the console (after a 15 minute wait).

Requesting a batch job using slurm
========================================================
Another option is to send your job to the server where it will be assigned to a node. One advantage of this process is that you don't have to remain logged on to the server while your job completes.

https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/R%20Howto%20for%20Gizmo.aspx

There are a lot of options and commands that will not be covered here.

Batch job using slurm - R script
========================================================
[A script for bash simulations](https://github.fhcrc.org/bmayer/cluster-presentation/blob/master/Code/test_batch_code.R)

Key differences from test_server_code.R:    
1. Removed the `foreach` method because it was too slow.
2. In addition to 10000 simulations, added 5 different parameter sets to cycle through (nested ldply).
3. Instead of saving all of the raw output, aggregated the results.

Took 1.2 minutes last time, with 2x more simulations, expect about 2ish minutes now.

Batch job using slurm - command line
========================================================
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
sbatch --cpus-per-task=6 --time=0-2 --wrap="R --no-save --no-restore < test_batch_code.R"
```

Important options here:    
1) We request a node with at least 6 cores for use using cpus-per-task=6    
2) We estimate the time we require (days-hours) with time=0-2    
3) There are additional options to email yourself updates (when the job finishes)   
4) Leave R options (no-save and no-restore) within the -wrap

Check on your jobs (with your hutchID):    
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
squeue -u bmayer
```

Sometimes the job will be queued for awhile. The wait time depends on the cores and the time requested. Waiting time seems to increase over the week (Monday morning shortest, Friday afternoon longest). You can cancel your jobs using scancel followed by the JOBID number (found using squeue).

```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
scancel 0000000x
```

Batch job using slurm
========================================================
An output file is created for each batch job (it will look like slurm-JOBID#.out, unless designated otherwise). The output file is updated as the job runs so you can see what code is currently being executed. If the job terminated early because of a bug, that will be the last line written to the .out file. You can inspect this code using `less`.    

```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
less slurm-0000000x.out
```

After the job is finished, can quickly preview results:
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
less batch_results.csv
```

Additional parallelization options
========================================================
Each job is sent to a node where the cores are accessed for your program. Additionally, you have access to a lot of nodes (120?). Taking advantage of this can be very powerful:
- Simplest method: Just send mulitple jobs to the server (rerun the script and make sure you change output variable names in the R code)
- Write more complicated bash scripts that use loops: have the loop pass in values to R that can be used for different simulation settings.

Scripting loops
========================================================
A scripting loop can just repeatedly send jobs to the server but it can also pass values into R.
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
#!/bin/bash

for x in {1,2}; do
sbatch --cpus-per-task=6 --time=0-1 --wrap="R --no-save --no-restore '--args inf_set=$x' < test_loopbatch_code.R"
done
```

The '--args inf_set=$x' (single quotes required) defines the arguments to be passed (will be called inf_set in R). The $x grabs the values from x given in the loop command. Bash can be finicky, for example, don't add spaces between values in the loop.    

This code isn't for copying and pasting. Need to use a bash script file (.sh).    
[bash script for loop](https://github.fhcrc.org/bmayer/cluster-presentation/blob/master/Code/example_bash_loop.sh)

Scripting loops - passing values into R
========================================================

The following R code is necessary:
```{r, eval = F}
args<-(commandArgs(TRUE));
if(length(args)==0){
  print("No arguments supplied.")

}else{
  for(i in 1:length(args)){
    eval(parse(text=args[[i]]))
  }
  print(args)
}
```
This tells R to check if there were values passed in. The variable name will come from the bash script. Then you can use that input to select certain parameter ranges or change output file names:

```{r, eval = F}
#use the inputted values from the bash loop to reduce out subset (change output name)
if(inf_set == 1) infectivity_list = infectivity_list[1]
if(inf_set == 2) infectivity_list = infectivity_list[2]
out_file_name = paste("batch_results/batch_results_loop", inf_set, ".csv", sep = "") #output name varies by input; save in folder
```


Running the bash script
========================================================
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
./example_bash_loop.sh
```

If you get a weird permission error, may need to do the following:
```{r, eval = F, engine = "bash", out.width="1920px",height="1080px"} 
chmod u+x example_bash_loop.sh
```

Output from mulitple jobs
========================================================
If you are running multiple jobs using the same script, make sure the output file names change or they will just be overwritten.    

With multiple output files, it may be necessary to write a script to combine them.  Here is the script for this example.
```{r, eval = F}
#remember this has to be in the cluster-example folder to access the relative path "batch_results/"
library(plyr)

output_file_names = list.files("batch_results/")

output = ldply(output_file_names, function(file_name){
  read.csv(paste("batch_results/", file_name, sep = ""), stringsAsFactors = F)
})

write.csv(output, "combined_batchloop_output.csv", row.names = F)
```
[R script to combine output](https://github.fhcrc.org/bmayer/cluster-presentation/blob/master/Code/combine_batch_output.R)


Workflow using the servers
========================================================
A big drawback of using the server/command line is that the interactive RStudio experience is lost.    

Tips:
- Identify the output you want from your runs (.Rda files, csv's, plots)
- Find a workflow: find a comfortable way to quickly access your results and update code as needed (i.e., using NoMachine or Cyberduck to pass things back and forth)
- Anticipate bugs: don't let a 15 hour batch run go to waste because you wrote a write.csv command incorrectly at the end. Run an example batch first that should run quickly to check for bugs. Consider outputting results or printing status reports periodically.
- Parallelizing with nodes (looping batch scripts) means you may have to combine your output manually.

